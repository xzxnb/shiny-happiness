import typer
import torch
import json
import os
import dagshub
import torch_geometric as tg
import pytorch_lightning as pl
from pydantic import BaseModel
#from dgd.datasets import fo2_dataset
from DiGress.dgd.datasets import fo2_dataset
from gan.mlflow_utils import SafeMLFlowLogger
import re
from typing import Optional
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from graph_transformer import GF
app = typer.Typer(pretty_exceptions_enable=False)


def convert_wfomi_format_to_shiny(path: str) -> str:
    with open(path) as f:
        data = sorted(json.load(f))

    new_path = path.replace(".json", ".txt")

    with open(new_path, "w") as f:
        for idx, sample_as_list in enumerate(data):
            sample_as_list = sorted(sample_as_list)
            f.write(json.dumps(sample_as_list) + "\n")

    return new_path


@app.command()
def main(  # classify_two_mlns
    folder: str,
    seed: int = 0,
    gpu: int = 0,
    dry: bool = False,
    user: str = "color1_10k_GF",
):
    data_family = folder.split("json/")[1].split("/")[0].split("_")[0][:-1]
    domain_size = int(folder.split("domain")[1].split("/")[0])
    path_1 = convert_wfomi_format_to_shiny(f"{folder}/0.json")
    path_2 = convert_wfomi_format_to_shiny(f"{folder}/1.json")
    run_name = "-".join(folder.split("/")[2:])

    print(
        f"Training on {path_1=} vs. {path_2=}, {data_family}, {domain_size}, {run_name=}"
    )
    train(
        config_name="vs",
        train_path=path_1,
        domain_size=domain_size,
        data_family=data_family,
        data_name=None,
        generated_path=path_2,
        gpu=gpu,
        dry=dry,
        user=user,
        run_name=run_name,
        auto_train_ratio=0.5,
        seed=seed,
    )


# @app.command()
def main_classify_two_families(  # classify_two_mlns
    path_1: str,
    path_2: str,
    domain_size: int,
    data_family: str,
    gpu: int = 0,
    dry: bool = False,
    user: str = "peter",
):
    print(f"Training on {path_1=} vs. {path_2=}")
    train(
        config_name="vs",
        train_path=path_1,
        domain_size=domain_size,
        data_family=data_family,
        data_name=None,
        generated_path=path_2,
        gpu=gpu,
        dry=dry,
        user=user,
        run_name="test",
    )


def main_classify_generated(
    data_name: str,
    gpu: int = 0,
    dry: bool = False,
    user: str = "peter",
):
    with open(
        f"/home/jungpete/projects/deepgraphon/outputs/{data_name}.paths.txt"
    ) as f:
        generated_paths = [
            "/home/jungpete/projects/deepgraphon/" + x for x in json.load(f)
        ]

    number_match = re.search(r"\d+", data_name)
    domain_size = int(number_match.group())
    data_family = data_name.replace(str(domain_size), "")

    for generated_path in generated_paths:
        print(f"Training on {generated_path}")
        train(
            config_name=data_name,
            data_name=data_name,
            train_path=None,
            domain_size=domain_size,
            data_family=data_family,
            generated_path=generated_path,
            gpu=gpu,
            dry=dry,
            user=user,
        )


def train(
    config_name: str,
    data_name: Optional[str],
    train_path: Optional[str],
    domain_size: int,
    data_family: str,
    generated_path: str,
    gpu: int,
    dry: bool,
    user: str,
    run_name: str,
    auto_train_ratio: float,
    seed: int,
) -> None:
    assert data_name or train_path
    cfg = CFG(
        train=TrainConfig(batch_size=128, num_workers=16, n_train_data=-1),
        general=GeneralConfig(name=config_name),
    )
    pl.seed_everything(seed)

    datamodule = fo2_dataset.FO2DataModule(
        file_name=data_name,
        train_path=train_path,
        gen_file=generated_path,
        cfg=cfg,
        auto_train_ratio=auto_train_ratio,
    )
    datamodule.prepare_data()

    # Data generated by digress have one more "dummy" class.
    data_family_name_to_n_node_classes = {
        "deskmate-students": 1,
        "exists-friends-person": 3 if data_name else 2,
        "friends-person": 3 if data_name else 2,
        "random-person": 2 if data_name else 1,
        "weightedcolors-v": 5 if data_name else 4,
        "color": 3 if data_name else 3,
    }
    data_family_name_to_n_edge_classes = {
        "deskmate-students": 3,
    }

    # gnn = GNN(
    #     n_node_classes=data_family_name_to_n_node_classes[data_family],
    #     n_edge_classes=data_family_name_to_n_edge_classes.get(data_family, 1),
    #     domain_size=domain_size,
    #     out_channels=256,
    #     gnn_layer="DenseSAGEConv",
    #     depth=4,
    #     val_metrics_file=generated_path + f".val_metrics.{seed=}.multiedgefixed.json",
    # )
    gnn = GF(
        n_node_classes=data_family_name_to_n_node_classes[data_family],
        n_edge_classes=data_family_name_to_n_edge_classes.get(data_family, 1),
        domain_size=domain_size,
        val_metrics_file=generated_path + f".val_metrics.{seed=}.multiedgefixed.json",
    )

    if os.environ.get("DAGSHUB_REPO_OWNER") or os.environ.get("DAGSHUB_REPO_NAME"):
        dagshub.init(
            repo_owner=os.environ.get("DAGSHUB_REPO_OWNER"),
            repo_name=os.environ.get("DAGSHUB_REPO_NAME"),
            mlflow=True,
        )

    logger_mlflow = (
        SafeMLFlowLogger(
            run_name=run_name,
            experiment_name=f"SHINY - {user}",
            tags={
                "mlflow.note.content": f"""
{data_name=}
{train_path=}
{generated_path=}
{auto_train_ratio=}
""",
            },
            tracking_uri=os.environ.get("MLFLOW_TRACKING_URI", "http://localhost:5000"),
        )
        if not dry
        else None
    )
    if logger_mlflow is not None:
        logger_mlflow.log_hyperparams(
            {
                "config_name": config_name,
                "data_name": data_name,
                "train_path": train_path,
                "domain_size": domain_size,
                "data_family": data_family,
                "generated_path": generated_path,
                "run_name": run_name,
                "auto_train_ratio": auto_train_ratio,
                "seed": seed,
            }
        )

    trainer = pl.Trainer(
        max_epochs=1000,
        accelerator="cuda" if 0 <= gpu else "cpu",
        devices=[gpu] if 0 <= gpu else "auto",
        check_val_every_n_epoch=1,
        logger=[logger_mlflow] if logger_mlflow is not None else None,
        callbacks=[EarlyStopping(monitor="val_accuracy", mode="max", patience=10)],
    )
    trainer.fit(
        model=gnn,
        train_dataloaders=datamodule.dataloaders["train_and_gen"],
        val_dataloaders=datamodule.dataloaders["test_and_gen"],
    )


class TrainConfig(BaseModel):
    batch_size: int
    num_workers: int
    n_train_data: int


class GeneralConfig(BaseModel):
    name: str


class CFG(BaseModel):
    train: TrainConfig
    general: GeneralConfig


class GNN(pl.LightningModule):
    def __init__(
        self,
        n_node_classes: int,
        n_edge_classes: int,
        domain_size: int,
        out_channels: int,
        gnn_layer: str,
        depth: int,
        allow_floats_adj: bool = False,
        val_metrics_file: str = None,
    ):
        super().__init__()
        self.allow_floats_adj = allow_floats_adj
        self.val_metrics_file = val_metrics_file

        self.input_layer = getattr(tg.nn, gnn_layer)(
            in_channels=n_node_classes,
            out_channels=out_channels,
        )
        self.hidden_layers = torch.nn.ModuleList(
            [
                getattr(tg.nn, gnn_layer)(
                    in_channels=out_channels,
                    out_channels=out_channels,
                )
                for _ in range(depth - 1)
            ]
        )
        self.aggregator = torch.nn.Linear(out_channels, 1)
        self.classifier = torch.nn.Linear(domain_size * n_edge_classes, 1)

    def process_subgraph(self, node_features, adj_matrix):
        assert node_features.dim() == 3 and node_features.size(1) == adj_matrix.size(
            1
        ), f"{node_features.shape=}, {adj_matrix.shape=}, [batch_size, n_nodes, n_feat]"
        assert adj_matrix.dim() == 3 and adj_matrix.size(1) == adj_matrix.size(
            2
        ), f"{adj_matrix.shape=}, [batch_size, n_nodes, n_nodes]"
        assert self.allow_floats_adj or bool(
            ((adj_matrix == 0) | (adj_matrix == 1)).all()
        ), "Adj matrix should be zeros and ones"

        x = self.input_layer(node_features, adj_matrix)
        x = torch.nn.functional.tanh(x)

        for layer in self.hidden_layers:
            x = layer(x, adj_matrix)
            x = torch.nn.functional.tanh(x)

        x = self.aggregator(x).squeeze(-1)
        x = torch.nn.functional.tanh(x)

        return x

    def forward(self, node_features, edge_type_to_adj_matrix):
        subgraphs = [
            self.process_subgraph(node_features, adj_matrix)
            for adj_matrix in edge_type_to_adj_matrix.values()
        ]
        x = torch.cat(subgraphs, dim=-1)

        x = self.classifier(x).reshape(-1)
        x = torch.nn.functional.sigmoid(x)

        return x

    def training_step(self, batch, batch_idx):
        out = self(batch.node_feats, batch.edge_type_to_adj_matrix)
        loss = torch.nn.functional.binary_cross_entropy(out, batch.gan_y)
        self.log("train_loss", loss, on_epoch=True)
        # self.log("train_loss", loss, batch_size=len(batch.gan_y))
        # self.log(
        #     "train_accuracy", accuracy(out, batch.gan_y), batch_size=len(batch.gan_y)
        # )
        # self.log(
        #     "train_precision", precision(out, batch.gan_y), batch_size=len(batch.gan_y)
        # )
        # self.log("train_recall", recall(out, batch.gan_y), batch_size=len(batch.gan_y))
        return loss

    def validation_step(self, batch, batch_idx):
        out = self(batch.node_feats, batch.edge_type_to_adj_matrix)
        loss = torch.nn.functional.binary_cross_entropy(out, batch.gan_y)
        self.log("val_loss", loss, batch_size=len(batch.gan_y))
        val_accuracy = accuracy(out, batch.gan_y)
        val_precision = precision(out, batch.gan_y)
        val_recall = recall(out, batch.gan_y)
        self.log(
            "val_accuracy", val_accuracy, prog_bar=True, batch_size=len(batch.gan_y)
        )
        self.log("val_precision", val_precision, batch_size=len(batch.gan_y))
        self.log("val_recall", val_recall, batch_size=len(batch.gan_y))
        return {
            "val_accuracy": val_accuracy,
            "val_precision": val_precision,
            "val_recall": val_recall,
        }

    def validation_epoch_end(self, outputs):
        val_accuracy = sum(x["val_accuracy"] for x in outputs) / len(outputs)
        val_precision = sum(x["val_precision"] for x in outputs) / len(outputs)
        val_recall = sum(x["val_recall"] for x in outputs) / len(outputs)

        # Initialize highest values if they don't exist
        if not hasattr(self, "highest_val_accuracy"):
            self.highest_val_accuracy = 0.0
            self.highest_val_precision = 0.0
            self.highest_val_recall = 0.0

        # Update highest values
        self.highest_val_accuracy = max(self.highest_val_accuracy, val_accuracy)
        self.highest_val_precision = max(self.highest_val_precision, val_precision)
        self.highest_val_recall = max(self.highest_val_recall, val_recall)

        # Save the highest values
        with open(self.val_metrics_file, "w") as f:
            json.dump(
                {
                    "val_accuracy": self.highest_val_accuracy,
                    "val_precision": self.highest_val_precision,
                    "val_recall": self.highest_val_recall,
                },
                f,
                indent=2,
            )

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)
        return optimizer


def accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:
    pred_hard = (pred > 0.5).float()
    return (pred_hard == target).float().mean().item()


def precision(pred: torch.Tensor, target: torch.Tensor) -> float:
    pred_hard = (pred > 0.5).float()
    true_positives = ((pred_hard == 1) & (target == 1)).float().sum().item()
    predicted_positives = (pred_hard == 1).float().sum().item()
    return true_positives / predicted_positives if predicted_positives > 0 else 0.0


def recall(pred: torch.Tensor, target: torch.Tensor) -> float:
    pred_hard = (pred > 0.5).float()
    true_positives = ((pred_hard == 1) & (target == 1)).float().sum().item()
    actual_positives = (target == 1).float().sum().item()
    return true_positives / actual_positives if actual_positives > 0 else 0.0


if __name__ == "__main__":
    # from torchviz import make_dot
    # import matplotlib.pyplot as plt
    # from torchinfo import summary
    # # 初始化模型和输入（同上）
    # model = GF(
    #     n_node_classes=3,
    #     n_edge_classes=1,
    #     domain_size=10,
    #     val_metrics_file=f".val_metrics.multiedgefixed.json"
    #            )
    # input_feat = torch.zeros((128, 10, 4), dtype=torch.float32)
    # for i in range(128):
    #     for j in range(10):
    #         seed = torch.tensor([i, j]).sum().item()  # 基于循环索引生成种子
    #         torch.manual_seed(seed)
    #         feat_idx = torch.randint(low=1, high=4, size=(1,)).item()
    #         input_feat[i,j,feat_idx] = 1
    # adj_matrix_b = torch.randint(
    #     low=0,  # 最小值（包含）
    #     high=2,  # 最大值（不包含，因此实际取 0 或 1）
    #     size=(128, 10, 10),
    #     dtype=torch.float32  # 若需要浮点型（如用于计算）
    # )
    # for i in range(128):
    #     for j in range(10):
    #         for k in range(j+1):
    #             if k !=j:
    #                 adj_matrix_b[i,j,k] = adj_matrix_b[i,k,j]
    #             else:
    #                 adj_matrix_b[i, j, k] = 0
    #
    # adj_matrix={0:adj_matrix_b}
    # summary(model, input_data=[input_feat, adj_matrix], depth=5)

    app()
